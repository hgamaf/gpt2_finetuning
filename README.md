# Fine-tuning do GPT-2 com Dataset Alpaca

[![Python 3.12](https://img.shields.io/badge/Python-3.12-blue.svg)](https://www.python.org/downloads/release/python-3120/)
[![PyTorch 2.0](https://img.shields.io/badge/PyTorch-2.0-red.svg)](https://pytorch.org/)
[![Transformers 4.30](https://img.shields.io/badge/Transformers-4.30-yellow.svg)](https://huggingface.co/docs/transformers/index)
[![Datasets 2.12](https://img.shields.io/badge/Datasets-2.12-green.svg)](https://huggingface.co/docs/datasets/index)
[![Plotly 5.18](https://img.shields.io/badge/Plotly-5.18-purple.svg)](https://plotly.com/python/)
[![Dash 2.14](https://img.shields.io/badge/Dash-2.14-orange.svg)](https://dash.plotly.com/)

## ğŸ“Š Fluxo do Projeto

```mermaid
graph TD
    A[Dataset Alpaca] --> B[AnÃ¡lise ExploratÃ³ria]
    B --> C[DivisÃ£o Estratificada]
    C --> D[Dataset de Treino]
    C --> E[Dataset de Teste]
    D --> F[Fine-tuning GPT-2]
    E --> G[AvaliaÃ§Ã£o do Modelo]
    F --> G
    G --> H[Resultados]
```

## ğŸ“ Estrutura do Projeto

```
gpt2_finetuning/
â”œâ”€â”€ data/                    # Dados e scripts de processamento
â”‚   â”œâ”€â”€ train_dataset.parquet    # Dataset de treino
â”‚   â””â”€â”€ test_dataset.parquet     # Dataset de teste
â”œâ”€â”€ eda/                    # AnÃ¡lise ExploratÃ³ria de Dados
â”‚   â”œâ”€â”€ analyze_fulldataset.py   # Script de anÃ¡lise
â”‚   â””â”€â”€ stats/                   # Resultados das anÃ¡lises
â”œâ”€â”€ src/                    # CÃ³digo fonte
â”‚   â”œâ”€â”€ data/                   # Processamento de dados
â”‚   â”œâ”€â”€ model/                  # Modelo e treinamento
â”‚   â””â”€â”€ utils/                  # UtilitÃ¡rios
â”œâ”€â”€ requirements.txt        # DependÃªncias do projeto
â””â”€â”€ README.md              # DocumentaÃ§Ã£o
```

## ğŸ“Š EstatÃ­sticas do Dataset

### Dataset Original
- **Total de Exemplos**: ~52,000
- **Exemplos com Input**: ~40% (20,800)
- **Exemplos sem Input**: ~60% (31,200)

### Datasets de Treino e Teste
- **Treino**: 3,000 exemplos
- **Teste**: 3,000 exemplos
- **Total**: 6,000 exemplos (~11.5% do original)

## ğŸ› ï¸ InstalaÃ§Ã£o

1. Instale o gerenciador de pacotes `uv`:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

2. Compile as dependÃªncias:
```bash
uv pip compile pyproject.toml -o requirements.txt
```

3. Instale as dependÃªncias:
```bash
uv pip install -r requirements.txt
```

## ğŸ”¬ Metodologia de Amostragem

O dataset Alpaca original contÃ©m aproximadamente 52,000 exemplos. Para garantir uma avaliaÃ§Ã£o robusta do modelo, realizei uma amostragem estratificada considerando dois aspectos principais:

1. **Comprimento das InstruÃ§Ãµes**: Dividi as instruÃ§Ãµes em quartis para garantir uma distribuiÃ§Ã£o uniforme de exemplos curtos, mÃ©dios e longos. Isso Ã© crucial porque diferentes comprimentos de instruÃ§Ã£o podem exigir diferentes capacidades do modelo.

2. **PresenÃ§a de Input Adicional**: Estratifiquei tambÃ©m com base na presenÃ§a ou ausÃªncia de input adicional, mantendo a proporÃ§Ã£o original do dataset. Isso Ã© importante porque exemplos com input adicional geralmente requerem um processamento diferente do modelo.

Optei por uma amostra de 3,000 exemplos para treinamento e 3,000 para teste (totalizando 6,000 exemplos, aproximadamente 11.5% do dataset original) por algumas razÃµes:

- O dataset Alpaca contÃ©m uma grande variedade de tipos de instruÃ§Ãµes (perguntas factuais, solicitaÃ§Ãµes criativas, tarefas de raciocÃ­nio, etc.)
- Se eu usasse amostragem aleatÃ³ria simples, correria o risco de ter sub-representaÃ§Ã£o ou sobre-representaÃ§Ã£o de certos tipos de instruÃ§Ãµes
- Isso poderia levar a avaliaÃ§Ãµes enviesadas do desempenho do modelo
- Com a estratificaÃ§Ã£o, garanto que o modelo serÃ¡ testado em uma amostra representativa de todos os tipos de instruÃ§Ãµes

A implementaÃ§Ã£o tÃ©cnica foi feita usando o `train_test_split` do scikit-learn, aplicando a estratificaÃ§Ã£o nas caracterÃ­sticas mencionadas. Isso garante que tanto o conjunto de treinamento quanto o de teste mantenham as mesmas proporÃ§Ãµes do dataset original em relaÃ§Ã£o a essas caracterÃ­sticas.

## ğŸ¤– ExecuÃ§Ã£o do Modelo

O processo de geraÃ§Ã£o de respostas foi implementado no script `task_one/run_instructions.py` e segue as seguintes etapas:

### 1. ConfiguraÃ§Ã£o do Modelo
- UtilizaÃ§Ã£o do modelo GPT-2 small (ComCom/gpt2-small)
- ConfiguraÃ§Ã£o do pipeline de geraÃ§Ã£o de texto com:
  - `max_length=200` (limite de tokens por resposta)
  - `temperature=0.7` (controle de criatividade)
  - `top_p=0.9` (nucleus sampling)
  - `truncation=True` (truncamento explÃ­cito)
  - Suporte automÃ¡tico para GPU quando disponÃ­vel

### 2. Processamento das InstruÃ§Ãµes
- Carregamento do dataset de teste (3,000 exemplos)
- GeraÃ§Ã£o de resposta para cada instruÃ§Ã£o
- Armazenamento dos resultados em formato estruturado:
  - InstruÃ§Ã£o original
  - Input (quando presente)
  - Output original
  - Output gerado
  - Ãndice do exemplo

### 3. Salvamento dos Resultados
- CriaÃ§Ã£o de diretÃ³rio `results` para armazenamento
- Salvamento parcial a cada 100 exemplos (medida de seguranÃ§a)
- GeraÃ§Ã£o de arquivo final com timestamp
- Formato JSON com indentaÃ§Ã£o para legibilidade

**OBS**: O salvamento parcial a cada 100 exemplos foi implementado como medida de seguranÃ§a para:
- Evitar perda de dados em caso de interrupÃ§Ã£o
- Permitir monitoramento do progresso
- Facilitar a retomada do processo se necessÃ¡rio

## ğŸ“ˆ AvaliaÃ§Ã£o dos Resultados

Para avaliar a qualidade das respostas geradas pelo modelo, utilizei um conjunto abrangente de mÃ©tricas implementadas no script `task_one/evaluation/evaluate_responses.py`. A avaliaÃ§Ã£o foi realizada comparando as respostas geradas com as respostas originais do dataset Alpaca.

### MÃ©tricas Utilizadas

1. **BLEU Score**
   - Mede a similaridade entre a resposta gerada e a resposta original
   - PontuaÃ§Ã£o de 0 a 1, onde valores mais altos indicam maior similaridade
   - InterpretaÃ§Ã£o:
     - BOM (â‰¥ 0.6): Alta similaridade com a resposta original
     - MÃ‰DIO (â‰¥ 0.3): Similaridade moderada
     - RUIM (< 0.3): Baixa similaridade

2. **ROUGE Scores**
   - Avalia a sobreposiÃ§Ã£o de palavras e sequÃªncias entre as respostas
   - MÃ©tricas calculadas:
     - ROUGE-1: SobreposiÃ§Ã£o de palavras Ãºnicas
     - ROUGE-2: SobreposiÃ§Ã£o de pares de palavras
     - ROUGE-L: SobreposiÃ§Ã£o de sequÃªncias mais longas
   - InterpretaÃ§Ã£o:
     - BOM (â‰¥ 0.7): Alta sobreposiÃ§Ã£o
     - MÃ‰DIO (â‰¥ 0.4): SobreposiÃ§Ã£o moderada
     - RUIM (< 0.4): Baixa sobreposiÃ§Ã£o

3. **Cosine Similarity**
   - Mede a similaridade semÃ¢ntica entre os embeddings das respostas
   - Utiliza o modelo de embeddings do spaCy para representaÃ§Ã£o vetorial
   - InterpretaÃ§Ã£o:
     - BOM (â‰¥ 0.8): Alta similaridade semÃ¢ntica
     - MÃ‰DIO (â‰¥ 0.5): Similaridade moderada
     - RUIM (< 0.5): Baixa similaridade

4. **AnÃ¡lise de Comprimento**
   - Compara o comprimento das respostas geradas com as originais
   - Calcula a razÃ£o entre comprimentos (gerado/original)
   - InterpretaÃ§Ã£o:
     - BOM (0.8-1.2): Comprimento similar ao original
     - MÃ‰DIO (0.5-1.5): Comprimento moderadamente diferente
     - RUIM (< 0.5 ou > 1.5): Comprimento muito diferente

### Resultados Obtidos

Os resultados da avaliaÃ§Ã£o foram salvos em formato JSON no diretÃ³rio `task_one/evaluation/results/`. O relatÃ³rio inclui:

- EstatÃ­sticas descritivas para cada mÃ©trica
- DistribuiÃ§Ã£o das pontuaÃ§Ãµes
- AnÃ¡lise qualitativa baseada nos critÃ©rios de interpretaÃ§Ã£o
- Exemplos de respostas com diferentes nÃ­veis de qualidade

A anÃ¡lise detalhada dos resultados serÃ¡ incluÃ­da apÃ³s a execuÃ§Ã£o completa do processo de avaliaÃ§Ã£o.

## ğŸ“Š AnÃ¡lise ExploratÃ³ria

O script `analyze_fulldataset.py` realiza uma anÃ¡lise completa do dataset, incluindo:

1. **EstatÃ­sticas de Comprimento**:
   - MÃ©dia, mediana, desvio padrÃ£o
   - MÃ­nimo e mÃ¡ximo
   - Para instruÃ§Ãµes, outputs e inputs

2. **AnÃ¡lise de Inputs**:
   - Total de exemplos
   - ProporÃ§Ã£o com/sem input
   - Percentual de exemplos com input

3. **AnÃ¡lise de InstruÃ§Ãµes**:
   - Top 5 verbos mais frequentes
   - DistribuiÃ§Ã£o por tipo de instruÃ§Ã£o

4. **AnÃ¡lise de Legibilidade**:
   - Ãndice Flesch Reading Ease
   - EstatÃ­sticas de legibilidade

5. **AnÃ¡lise de TÃ³picos**:
   - IdentificaÃ§Ã£o de 5 tÃ³picos principais
   - Palavras mais relevantes por tÃ³pico

## ğŸ“š ReferÃªncias

- [Dataset Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)
- [DocumentaÃ§Ã£o do GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)
- [Flesch, R. (1948). A new readability yardstick. Journal of Applied Psychology, 32(3), 221-233.](https://psycnet.apa.org/record/1948-05052-001)
